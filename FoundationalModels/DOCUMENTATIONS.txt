Below is thorough documentation for your project, which uses embeddings generated by a Vision Transformer (ViT) model (specifically SpectralGPT from the IEEE TPAMI repository) to train a Multilayer Perceptron (MLP) for regression to predict organic carbon (OC) content. The documentation covers the script’s purpose, methodology, output structure, and how to extend it for MLP regression training, based on the provided code and output sample.
Project Documentation: Using SpectralGPT Embeddings for Organic Carbon Regression with an MLP
Overview
This project leverages the SpectralGPT model (from https://github.com/danfenghong/IEEE_TPAMI_SpectralGPT) to generate embeddings from multi-year geospatial raster data, which are then saved as Parquet files. These embeddings represent spatial and temporal features (e.g., elevation, LAI, LST) for locations in Bavaria, Germany, extracted from datasets like LUCAS and LFU/LfL. The ultimate goal is to use these embeddings as input features to train a Multilayer Perceptron (MLP) for regression to predict organic carbon (OC) content in soil.
The provided script (getParquet.py) processes raster data, generates embeddings using a pre-trained SpectralGPT Vision Transformer (ViT), and saves them in a structured format. This documentation describes the script’s functionality, the output format, and outlines how to proceed with training an MLP for regression.
Objectives
Generate Embeddings: Use SpectralGPT to extract meaningful feature representations from multi-year raster data (elevation and environmental bands like LAI, LST, etc.).
Data Storage: Save embeddings, coordinates, and OC values in Parquet files for efficient storage and retrieval.
Regression Task: Train an MLP to predict OC values using the embeddings as input features.
Methodology
1. Data Preparation
Input Data: The script uses raster datasets stored in /home/vfourel/SOCProject/SOCmapping/Data, including:
Static data: Elevation.
Yearly data: LAI, LST, MODIS_NPP, SoilEvaporation, TotalEvapotranspiration.
Seasonal data: Similar bands with seasonal granularity (though the script focuses on yearly data).
Configuration: Defined in config.py:
TIME_BEGINNING: 2012.
TIME_END: 2023.
time_before: 5 years (2007–2011 as historical context).
bands_dict: Maps indices to band names (e.g., 0: Elevation, 1: LAI, etc.).
imageSize: 96x96 pixels.
MAX_OC: 150 (filters out samples with OC > 150).
Dataset Creation: 
MultiRasterDatasetMultiYears: Custom dataset class to load raster data and coordinates.
create_balanced_dataset: Balances the dataset by binning OC values and resampling to ensure a homogeneous distribution.
2. Model Setup
Model: SpectralGPT-based Vision Transformer (default: vit_base_patch8).
Loaded from a pre-trained checkpoint (SpectralGPT+.pth).
Fine-tuned by adjusting weights and embeddings (e.g., interpolate_pos_embed).
Device: CUDA if available, otherwise CPU.
Parameters: 
drop_path: 0.1 (dropout rate for regularization).
nb_classes: 62 (originally for classification, repurposed for embeddings).
3. Embedding Generation
Data Transformation (transform_data):
Input: Stacked tensor [B, total_steps, H, W] (e.g., [256, 26, 96, 96]) and metadata [B, total_steps, 2].
Process:
Extracts elevation data (assumed at metadata [0,0]).
Sorts remaining bands by year (descending) and organizes into channels (5 bands) and time steps (5 years).
Outputs:
elevation_instrument: [B, 1, H, W].
remaining_tensor: [B, num_channels, time_before, H, W] (e.g., [256, 5, 5, 96, 96]).
metadata_strings: List of band-year tags (e.g., LAI_2008).
Embedding Extraction (process_batch_to_dataframe):
For each sample:
Elevation embedding: Processed through model.patch_embed (expanded to 3 channels for ViT compatibility).
Channel-time embeddings: For each channel-time pair (5x5 = 25 combinations), processed similarly.
Output: DataFrame with columns longitude, latitude, organic_carbon, elevation_embedding, channel_time_embeddings.
4. Output Storage
Function: save_to_parquet.
Path: /fast/vfourel/SOCProject/run_<timestamp> (e.g., run_20250209_163939).
Files:
Parquet files: batch_<idx>_size_<batch_size>.parquet (e.g., batch_0001_size_256.parquet).
Documentation: run_<timestamp>_documentation.txt.
Output Description
The script outputs Parquet files containing processed data for each batch. Below is an analysis of the sample output from batch_0001_size_256.parquet:
Output Sample
DataFrame Info:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 256 entries, 0 to 255
Data columns (total 5 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   longitude                256 non-null    float64
 1   latitude                 256 non-null    float64
 2   organic_carbon           256 non-null    float64
 3   elevation_embedding      256 non-null    object 
 4   channel_time_embeddings  256 non-null    object 
dtypes: float64(3), object(2)
memory usage: 10.1+ KB

First few rows:
   longitude   latitude  organic_carbon  elevation_embedding  channel_time_embeddings
0  13.319518  48.155585         71.2     [[-8.7746, ...]]    {'LAI_2008': [[-7.5074, ...]], ...}
1  10.624857  ...                   ...     ...                 {'LAI_2008': None, ...}
2  10.570964  ...                   ...     ...                 {'LAI_2008': None, ...}
3  11.926018  ...                   ...     ...                 {'LAI_2008': None, ...}
4  11.279509  ...                   ...     ...                 {'LAI_2008': None, ...}

Sample row (first row):
longitude: 13.319518
latitude: 48.155585
organic_carbon: 71.2
elevation_embedding: [[-8.7746000289917, -12.59272289276123, -0.981..., ...]]  # 768 elements
channel_time_embeddings: {
    'LAI_2008': [[-7.507430076599121, -1.09816849..., ...]],  # 768 elements
    'LAI_2007': ...,
    'LST_2008': ...,
    ...  # 25 total entries (5 channels x 5 time steps)
}
Column Details
longitude (float64):
Geographic longitude of the sample location.
Example: 13.319518.
latitude (float64):
Geographic latitude of the sample location.
Example: 48.155585.
organic_carbon (float64):
Target variable: Organic carbon content in soil (g/kg).
Example: 71.2.
elevation_embedding (object):
Embedding vector for elevation data, extracted via model.patch_embed.
Shape: List of lists (e.g., [[...]]), typically 768 elements for a ViT base model.
Example: [-8.7746, -12.5927, ...].
channel_time_embeddings (object):
Dictionary mapping band-year tags to embeddings.
Keys: <band>_<year> (e.g., LAI_2008, LST_2007).
Values: Embedding lists (768 elements each).
Size: Up to 25 entries (5 bands × 5 years), though some may be None if data is missing.
Observations
Batch Size: 256 samples per file, consistent with DataLoader(batch_size=256).
Embedding Size: Likely 768 (standard for ViT base models), though not fully shown in the output.
Missing Data: Some channel_time_embeddings entries are None, indicating missing raster data for certain band-year combinations.
Extending to MLP Regression
Objective
Train an MLP to predict organic_carbon using elevation_embedding and channel_time_embeddings as features.
Steps
1. Data Preprocessing
Load Parquet Files:
python
import pandas as pd
import glob

parquet_files = glob.glob("/fast/vfourel/SOCProject/run_20250209_163939/*.parquet")
dfs = [pd.read_parquet(f) for f in parquet_files]
data = pd.concat(dfs, ignore_index=True)
Flatten Embeddings:
Convert elevation_embedding and channel_time_embeddings into a single feature vector per sample.
Handle None values (e.g., impute with zeros or exclude).
python
import numpy as np

def flatten_embeddings(row):
    elev = np.array(row['elevation_embedding']).flatten()  # 768 elements
    cte = row['channel_time_embeddings']
    cte_flat = []
    for band in ['Elevation', 'LAI', 'LST', 'MODIS_NPP', 'SoilEvaporation', 'TotalEvapotranspiration']:
        for year in range(2007, 2012):  # Adjust based on time_before
            key = f"{band}_{year}"
            emb = cte.get(key)
            cte_flat.extend(np.zeros(768) if emb is None else np.array(emb).flatten())
    return np.concatenate([elev, cte_flat])  # 768 + (5 bands * 5 years * 768) = 19,968 features

X = np.stack(data.apply(flatten_embeddings, axis=1))
y = data['organic_carbon'].values
2. Split Data
Use train/validation split (e.g., 80/20).
python
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
3. Define MLP Model
Use PyTorch for flexibility.
python
import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_size=19968, hidden_size=512, output_size=1):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size // 2, output_size)
        )
    
    def forward(self, x):
        return self.layers(x)

model = MLP().to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
4. Training
Loss: Mean Squared Error (MSE) for regression.
Optimizer: Adam.
python
from torch.utils.data import TensorDataset, DataLoader

# Convert to tensors
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)
X_val_tensor = torch.FloatTensor(X_val)
y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
epochs = 50
for epoch in range(epochs):
    model.train()
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(model.device), batch_y.to(model.device)
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")
5. Evaluation
Metrics: R² score, RMSE.
python
from sklearn.metrics import r2_score, mean_squared_error

model.eval()
with torch.no_grad():
    y_pred = model(X_val_tensor.to(model.device)).cpu().numpy()
    r2 = r2_score(y_val, y_pred)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    print(f"R²: {r2:.4f}, RMSE: {rmse:.4f}")
Future Improvements
Feature Selection: Reduce dimensionality (e.g., PCA) to handle the large feature space (19,968 features).
Data Imputation: Address None values in channel_time_embeddings more robustly.
Hyperparameter Tuning: Optimize MLP architecture and training parameters using grid search or libraries like Optuna.
Cross-Validation: Implement k-fold cross-validation for robustness.
Conclusion
This project successfully generates embeddings from SpectralGPT for geospatial data and stores them in a structured format. By flattening these embeddings and training an MLP, it provides a pipeline for OC regression. The documentation and code above offer a complete starting point for further experimentation and deployment.
Let me know if you’d like me to refine any section or assist with implementation!
